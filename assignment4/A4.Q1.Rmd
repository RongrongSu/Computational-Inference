---
title: "Q1"
output: word_document
---
(a)
The likelihood function is:
$$ f(x|\lambda) = \prod_{i=1}^n \frac{\alpha {x_i}^{(\alpha-1)}}{\gamma} e^{-\frac{x_i ^{\alpha}}{\gamma}}  $$
And the prior distribution is:
$$ g(\gamma|\lambda,\beta) = \frac{\beta^\lambda}{\Gamma(\lambda)}(\frac{1}{\gamma})^{(\lambda+1)} exp^{-\frac{\beta}{\gamma}}   $$


So the posterior distribution is: 
$$
\begin{align}
\pi(\gamma|x) &\propto f(x|\lambda) g(\gamma|\lambda,\beta)  \\ 
&\propto \frac{\alpha^n \prod x_i ^{(\alpha-1)}}{\gamma^n} e^{-\frac{\sum x_i + ^{\alpha} + \beta}{\gamma}} \frac{\beta^\lambda}{\Gamma(\lambda)}(\frac{1}{\gamma})^{(\lambda+1)} exp^{-\frac{\beta}{\gamma}} \\
&\propto \gamma^{-({n+\lambda+1})} e^{-\frac{\sum(x_i^\alpha + \beta)}{\gamma}}
\end{align}
$$

So the posterior density is:
$$inv\Gamma(n+\lambda, \sum_{i=1} ^{n} (x_i ^\alpha +\beta))  $$

(b)

```{r}
library(invgamma)
set.seed(440)
data = rweibull(24, shape=1/2, scale=64)
n = length(data)
p = matrix(0, nrow = 3, ncol = 2) 
p[1,] = c(1,10)
p[2,] = c(10,1)
p[3,] = c(NA,NA)
m = p+cbind(rep(n,3), rep(sum(data^0.5),3)) 
m[3,1] = n-1
m[3,2] = sum(data^0.5)
alpha = 0.05
level = c(alpha/2, 1-alpha/2)
```

```{r}
rr = rbind(qinvgamma(level,shape = m[1,1],rate = m[1,2]),
           qinvgamma(level,shape = m[2,1],rate = m[2,2]),
           qinvgamma(level,shape = m[3,1],rate = m[3,2]))

result=as.matrix(cbind(p,m,m[,2]/(m[,1]-1),rr))
dimnames(result) =  list(c("1", "2", "3"), c("Prior1", "Prior2", "Post1", "Post2", "Mean", "Lower", "Upper"))

result
```

```{r}

xx=seq(0,30,0.001)

plot(xx, dinvgamma(xx, shape = 1, rate = 10), type="l", main = "Priors",ylim = c(0,1) )
lines(xx, dinvgamma(xx, shape = 10, rate = 1), type ="l",col = 'red')
lines(xx, rep(1,length(xx)), type ="l", col = 'blue')


```

```{r}
xx=seq(0,30,0.01)
plot(xx, dinvgamma(xx, shape=m[1,1], rate =  m[1,2]), type = "l", main ="Posteriors",ylim = c(0,0.5))
lines(xx, dinvgamma(xx, shape=m[2,1], rate = m[2,2]), type = "l",col = 'red')
lines(xx, dinvgamma(xx, shape=m[3,1], rate = m[3,2]), type = "l", col = 'blue')

```

(c)

```{r}
ga_est = 1/n*sum(data^0.5)

loglik<-function(gamma = NULL, datax = NULL){
  val = numeric(length(gamma))
  n = length(data)
  for (i in 1:length(gamma)){
    val[i] = n*log(0.5) + sum(log(datax)) -n*log(gamma)- sum(datax^0.5)/gamma
    
  }
  return (val)
}


loglik.ratio <- function(x = NULL, datax = NULL, gamma.est = NULL, q0 = NULL){
  ratio = -2 * (loglik(gamma = x, datax = datax) - loglik(gamma = gamma.est, datax = datax))
  val = ratio - q0
  
  return(val)
  
}

temp1 = uniroot(f=loglik.ratio, interval = c(0.1, ga_est), datax = data, gamma.est = ga_est, q0=3.841459)

temp2 = uniroot(f = loglik.ratio, interval = c(ga_est,30), datax = data, gamma.est = ga_est, q0 = 3.841459)

loglikratio.ci = c(temp1$root, temp2$root)
loglikratio.ci
```

(d) 

```{r}
plot(x = c(1,1), y = loglikratio.ci,xlim = c(0,5),ylim = c(0,20),type = 'l')

for (i in 1:3){
lines(x = c(i,i)+1, y = rr[i,]) }

points(1:4, c(ga_est, m[,2]/(m[,1]-1)), pch = 20)
```

```{r}
r2 = rbind(loglikratio.ci, rr)

r2 = as.matrix(cbind(c(ga_est, m[,2]/(m[,1]-1)),r2))

dimnames(r2) = list(c('loglikelihood ratio', '1', '2', '3'))

r2
```


As can be seen the confidence intervals given by likelihood ratio method, model1($\lambda =1$, $\beta =10$), and model3($p(\lambda) =1$) are similar, which are wider than that given by model2($\lambda =10$, $\beta =1$)


(e)

The proir is 
$$ \gamma|x = inv\Gamma(23,225) $$

So the density is given by:
```{r}
prob <- function(x) {
  if (x<0) return (0)
  
  else return (dinvgamma(x, shape = 23, rate = 225))
  
}
```

```{r}
mcmc <- function(n = NULL, sigma = 1, x0){
  x = numeric(n)
  
  x[1] = x0
  
  for (i in 2:n){
    y = rnorm (1, x[i-1], sd = sigma)
    u = runif(1)
    accept = prob (y) / prob(x[i-1])
    
    if (u < accept) {x[i] = y}
    
    else {x[i] = x[i-1]}
    
  }
  
 return (x) 
  
}
```

(ii) 

```{r}
set.seed(440)

n = 10^4

x0 = 8.9

sigma = c(0.1, 1, 5, 20)

m2 = matrix (0, nrow = n, ncol = length(sigma))

for (i in 1:length(sigma)) {
  
  m2 [,i] = mcmc(n = n, sigma = sigma[i], x0=x0)
}

par(mfrow = c(1,length(sigma)), mar = 2.5 * c(1,1,1,0.1))

for (i in 1:length(sigma)) plot(m2[, i ], main = paste('sigma', sigma[i]), type='l')
```

```{r}
for (i in 1:length(sigma)) acf (m2[,i], main = paste('mcmc_sigma',sigma[i]) , lag.max = 20)
```

```{r}
a = rbind(sigma, 
          accept = apply(m2, 2, function(x){mean(x[-1] != x[-length(x)] )}),
          mean = apply(m2, 2, mean), 
          mixing = apply(m2, 2, function(x){mean(diff(x)^2)}), 
          apply(m2, 2,quantile, probs = c(0.025, 0.975)),4)
```

As can be seen the model with $\sigma = 5$ gives the best result. When sigma is smaller than 5, the plots shows large correlation, which indicates a larger number would be appropriate. However, the model with  $\sigma = 20$ has higher correlation compared with that with $\sigma = 5$. 
The true value of CI is (6.1336, 13.7014). So, model with $\sigma = 20$ and model with $\sigma = 5$ give the best estimate CI. 




